{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cda696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb\n",
    "# https://deeplizard.com/learn/video/HGeI30uATws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ea34c",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ae35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import osmnx as ox\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import gym\n",
    "import csv\n",
    "import folium\n",
    "\n",
    "from gym import Env\n",
    "from gym import utils\n",
    "from gym.spaces import Discrete, Box\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from IPython.display import clear_output\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371604f1",
   "metadata": {},
   "source": [
    "# Load Road Network as Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8cd776",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = Polygon([(-122.75593463884833, 38.48043857931626), (-122.75593463884833, 38.44276212966626), \n",
    "                    (-122.71602508838065, 38.44276212966626), (-122.71602508838065, 38.48043857931626)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ec295",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ox.graph.graph_from_polygon(boundary, network_type = 'drive')\n",
    "graph = graph.to_undirected()\n",
    "\n",
    "graph = ox.speed.add_edge_speeds(graph, precision = 3)\n",
    "graph = ox.speed.add_edge_travel_times(graph, precision = 3)\n",
    "\n",
    "nodes, edges = ox.graph_to_gdfs(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = ox.graph_to_gdfs(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = edges.drop(columns = ['osmid', 'bridge', 'oneway', 'lanes', 'maxspeed', \n",
    "                              'geometry', 'ref', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98297b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ox.plot_graph_folium(graph, popup_attribute = 'travel_time', weight = 3, color = '#3498DB')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b516941",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "\n",
    "for n in range(len(graph)):\n",
    "    id_list.append(nodes.iloc[n].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0126c3",
   "metadata": {},
   "source": [
    "# Define Sample Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ace30",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#E74C3C', '#3498DB', '#F7DC6F']\n",
    "\n",
    "sample_paths = []\n",
    "\n",
    "# Path 0: shortest path by length\n",
    "sample_paths.append(list(ox.distance.shortest_path(graph, 56102877, 1965862095, weight = 'length')))\n",
    "\n",
    "# Path 1: shortest path by travel time\n",
    "sample_paths.append(list(ox.distance.shortest_path(graph, 56102877, 1965862095, weight = 'travel_time')))\n",
    "\n",
    "# Path 2: manually defined path\n",
    "sample_paths.append([56102877, 6940021052, 56102875, 56043223, 56102872, 55973175, 56004921, 55950049, 55950051,\n",
    "                     55950052, 55950053, 56125038, 56078315, 56040934, 56084803, 56084797, 7049377524, 7049377523,\n",
    "                     7049377522, 56084795, 7049377535, 7049377534, 7049377521, 7049377518, 56116115, 56116919,\n",
    "                     56116921, 56116923, 8268741787, 56019308, 56021878, 7204303428, 56116927, 56157636, 56034980,\n",
    "                     56129707, 56054706, 56097568, 56129711, 56117906, 56140110, 56055340, 56173132, 56102741,\n",
    "                     56109019, 56027403, 56043321, 56101250, 56040774, 56017692, 56040783, 1965862095])\n",
    "\n",
    "ox.plot.plot_graph_routes(graph, sample_paths, route_colors = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9978b",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e26da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathEnv(gym.Env):\n",
    "    \n",
    "    # Define metadata\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    # Set path to test: 0, 1, 2\n",
    "    PATH = sample_paths[2]\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PathEnv, self).__init__()\n",
    "        \n",
    "        # Initialize agent starting node\n",
    "        self.agent_node = 489\n",
    "        \n",
    "        # Initialize previous agent node\n",
    "        self.prev_agent_node = self.agent_node\n",
    "        \n",
    "        # Initialize agent path\n",
    "        self.agent_path = [self.agent_node]\n",
    "        \n",
    "        # Initialize goal node\n",
    "        self.goal_node = 752\n",
    "        \n",
    "        # Initialize goal time\n",
    "        self.goal_time = self.get_path_time(self.PATH)\n",
    "        \n",
    "        # Initialize time elapsed\n",
    "        self.time_elapsed = 0\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        n_actions = len(graph)\n",
    "        self.action_space = Discrete(n_actions)\n",
    "        self.observation_space = Discrete(n_actions)\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        if action in self.get_neighbor_indices():\n",
    "            # Save previous distance to goal\n",
    "            prev_dist = self.get_distance_to_goal()\n",
    "            \n",
    "            # Update previous agent node, agent node\n",
    "            self.prev_agent_node = self.agent_node\n",
    "            self.agent_node = action\n",
    "\n",
    "            # Update agent path\n",
    "            self.agent_path.append(action)\n",
    "\n",
    "            # Update time elapsed\n",
    "            self.time_elapsed += (graph.edges[(id_list[self.prev_agent_node], id_list[self.agent_node], 0)]['travel_time'] + 30) / 60\n",
    "            \n",
    "            # Get new distance to goal\n",
    "            dist = self.get_distance_to_goal()\n",
    "            \n",
    "            if dist < prev_dist:\n",
    "                # Reward for getting closer to goal\n",
    "                reward = 1 \n",
    "            else:\n",
    "                # Penalize for going farther away from goal\n",
    "                reward = -1\n",
    "            \n",
    "        else:\n",
    "            # End episode if agent goes in a loop\n",
    "            if action == -1:\n",
    "                done = True\n",
    "                neighbors = self.get_neighbor_indices()\n",
    "                \n",
    "                if len(neighbors) == 1:\n",
    "                    if(neighbors[0] == self.prev_agent_node):\n",
    "                        # Penalize for reaching a dead end\n",
    "                        reward = -100\n",
    "            else:\n",
    "                print('Illegal action')\n",
    "                done = True\n",
    "\n",
    "\n",
    "        if self.goal_reached():\n",
    "            done = True\n",
    "            \n",
    "            # Reward for getting close to goal time\n",
    "            error = abs(self.goal_time - self.time_elapsed)\n",
    "            \n",
    "            if error < 2.5:\n",
    "                reward += 10000\n",
    "            \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        n_state = self.agent_node\n",
    "        return n_state, reward, done, info\n",
    "\n",
    "    def render(self, mode = 'human'):\n",
    "        if mode != 'human':\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        ox.plot.plot_graph_route(graph, self.get_id_path(self.agent_path))\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        # Initialize agent starting node\n",
    "        self.agent_node = 489\n",
    "        \n",
    "        # Initialize previous agent node\n",
    "        self.prev_agent_node = self.agent_node\n",
    "        \n",
    "        # Initialize agent path\n",
    "        self.agent_path = [self.agent_node]\n",
    "        \n",
    "        # Initialize goal node\n",
    "        self.goal_node = 752\n",
    "        \n",
    "        # Initialize goal time\n",
    "        self.goal_time = self.get_path_time(self.PATH)\n",
    "        \n",
    "        # Initialize time elapsed\n",
    "        self.time_elapsed = 0\n",
    "        \n",
    "        n_state = self.agent_node\n",
    "        \n",
    "        return n_state\n",
    "    \n",
    "    # Get indices of neighbors to current node\n",
    "    def get_neighbor_indices(self):\n",
    "        id = id_list[self.agent_node]\n",
    "\n",
    "        neighbors = list(graph.neighbors(id))\n",
    "        neighbor_indices = []\n",
    "\n",
    "        for i in range(len(id_list)):\n",
    "            for n in neighbors:\n",
    "                if n == id_list[i]:\n",
    "                    neighbor_indices.append(i)\n",
    "\n",
    "        return neighbor_indices\n",
    "    \n",
    "    # Get ID path from index path\n",
    "    def get_id_path(self, path):\n",
    "        id_path = path.copy()\n",
    "        \n",
    "        for n in range(len(id_path)):\n",
    "            id_path[n] = id_list[id_path[n]]\n",
    "        \n",
    "        return id_path\n",
    "    \n",
    "    # Get index path from ID path\n",
    "    def get_index_path(self, path):\n",
    "        index_path = path.copy()\n",
    "        \n",
    "        for n in range(len(index_path)):\n",
    "            index_path[n] = id_list.index(index_path[n])\n",
    "        \n",
    "        return index_path\n",
    "    \n",
    "    # Get path time of given path\n",
    "    def get_path_time(self, path):\n",
    "        path_time = 0\n",
    "        \n",
    "        for n in range(len(path) - 1):\n",
    "            path_time += (graph.edges[(path[n], path[n + 1], 0)]['travel_time'] + 30) / 60\n",
    "        \n",
    "        return path_time\n",
    "    \n",
    "    # Get path distance of given path\n",
    "    def get_path_distance(self, path):\n",
    "        path_dist = 0\n",
    "        \n",
    "        for n in range(len(path) - 1):\n",
    "            path_dist += graph.edges[(path[n], path[n + 1], 0)]['length']\n",
    "        \n",
    "        return path_dist\n",
    "    \n",
    "    # Get path accuracy info\n",
    "    def get_path_accuracy(self, s, a):\n",
    "        matching_nodes = 0\n",
    "        \n",
    "        for node in s:\n",
    "            if node in a:\n",
    "                matching_nodes += 1\n",
    "        \n",
    "        print('Matching nodes: {} / {}'.format(matching_nodes, len(s)))\n",
    "        print('Length difference:', abs(len(s) - len(a)))\n",
    "        \n",
    "        accuracy_score = (matching_nodes / len(s)) * 100 - abs(len(s) - len(a))\n",
    "        \n",
    "        print('Accuracy score:', accuracy_score)\n",
    "    \n",
    "    def get_matching_nodes(self, s, a):\n",
    "        \n",
    "        matching_nodes = 0\n",
    "        \n",
    "        for node in s:\n",
    "            if node in a:\n",
    "                matching_nodes += 1\n",
    "        \n",
    "        return matching_nodes / len(s)\n",
    "    \n",
    "    def get_length_diff(self, s, a):\n",
    "        return abs(len(s) - len(a))\n",
    "    \n",
    "    # Get agent's distance to goal\n",
    "    def get_distance_to_goal(self):\n",
    "        p = ox.distance.shortest_path(graph, id_list[self.agent_node], id_list[self.goal_node], weight = 'length')\n",
    "        \n",
    "        return self.get_path_distance(p)\n",
    "\n",
    "    def get_possible_actions(self):\n",
    "        neighbors = self.get_neighbor_indices()\n",
    "        tmp = neighbors.copy()\n",
    "\n",
    "        for n in neighbors:\n",
    "            if n in self.agent_path:\n",
    "                tmp.remove(n)\n",
    "        \n",
    "        return tmp\n",
    "    \n",
    "    def get_q_vals(self):\n",
    "        q_vals = []\n",
    "        possible_actions = self.get_possible_actions()\n",
    "        \n",
    "        for a in possible_actions:\n",
    "            q_vals.append(q_table[state, a])\n",
    "        \n",
    "        return q_vals\n",
    "    \n",
    "    def get_best_action(self):\n",
    "        possible_actions = self.get_possible_actions()\n",
    "        \n",
    "        if not possible_actions:\n",
    "            return -1\n",
    "        else:\n",
    "            q_vals = self.get_q_vals()\n",
    "        \n",
    "        return possible_actions[random.choice(np.argwhere(q_vals == np.max(q_vals)).flatten().tolist())]\n",
    "\n",
    "    # Returns true if agent has reached goal\n",
    "    def goal_reached(self):\n",
    "        if self.agent_node == self.goal_node:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3f373",
   "metadata": {},
   "source": [
    "# Validate Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b795a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PathEnv()\n",
    "check_env(env, warn = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9d1d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e7633",
   "metadata": {},
   "source": [
    "# Test Random Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c026ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PathEnv()\n",
    "\n",
    "episodes = 10\n",
    "max_steps = 100\n",
    "history = []\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    env.render()\n",
    "        \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        possible_actions = env.get_possible_actions()\n",
    "        \n",
    "        if not possible_actions:\n",
    "            obs, reward, done, info = env.step(-1)\n",
    "        else:\n",
    "            obs, reward, done, info = env.step(random.choice(possible_actions))\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        clear_output(wait = True)\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            print('Episode finished.')\n",
    "            print('Goal time:', round(env.goal_time, 2))\n",
    "            print('Time elapsed:', round(env.time_elapsed, 2))\n",
    "            print('Score:', round(score, 2))\n",
    "            time.sleep(2)\n",
    "            break\n",
    "            \n",
    "    if not done:\n",
    "        print('Goal not reached in {} steps.'.format(max_steps), 'Score:', round(score, 2))\n",
    "        time.sleep(2)\n",
    "    \n",
    "    history.append(score)\n",
    "    \n",
    "print('Average score:', round(sum(history) / len(history), 2))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ed9dd",
   "metadata": {},
   "source": [
    "# Create and Initialize Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d210b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f24f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab06d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((state_size, action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a93773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a8ca4",
   "metadata": {},
   "source": [
    "# Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 2000\n",
    "max_steps = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "discount_rate = 0.5\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688c0a5",
   "metadata": {},
   "source": [
    "# Implement Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [.01, .1, .2, .3, .4, .5, .6, .7, .8, .9, .99]\n",
    "gamma = [.5, .4, .3, .2, .1, .05]\n",
    "num_episodes = 2000\n",
    "max_steps = 1000\n",
    "\n",
    "data = open('RL_Data_Middle_Ojeet_Params.csv', 'w', newline = '')\n",
    "fieldnames = ['alpha', 'gamma', 'optimal_reward', 'matching_nodes', 'length_diff', 'agent_path']\n",
    "writer = csv.DictWriter(data, fieldnames = fieldnames)\n",
    "writer.writeheader()\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "for a in alpha:\n",
    "    for g in gamma:\n",
    "        env = PathEnv()\n",
    "        env.reset()\n",
    "        \n",
    "        action_size = env.action_space.n\n",
    "        state_size = env.observation_space.n\n",
    "        q_table = np.zeros((state_size, action_size))\n",
    "        \n",
    "        rewards_all_episodes = []\n",
    "        policy_history = []\n",
    "        path_history = []\n",
    "\n",
    "        # Q-Learning algorithm\n",
    "        for episode in range(num_episodes):\n",
    "            clear_output(wait = True)\n",
    "            print('alpha:', a)\n",
    "            print('gamma:', g)\n",
    "            print('Episode', episode, '/', num_episodes)\n",
    "\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            rewards_current_episode = 0\n",
    "\n",
    "            for step in range (max_steps):\n",
    "                exploration_rate_threshold = random.uniform(0, 1)\n",
    "\n",
    "                possible_actions = env.get_possible_actions()\n",
    "\n",
    "                if not possible_actions:\n",
    "                    action = -1\n",
    "                else:\n",
    "                    if exploration_rate_threshold > exploration_rate:\n",
    "                        action = env.get_best_action()\n",
    "                    else:\n",
    "                        action = random.choice(possible_actions)\n",
    "\n",
    "                n_state, reward, done, info = env.step(action)\n",
    "\n",
    "                # Update Q-table for Q(s, a)\n",
    "                q_table[state, action] = q_table[state, action] * (1 - a) + \\\n",
    "                    a * (reward + g * np.max(q_table[n_state, :]))\n",
    "\n",
    "                state = n_state\n",
    "                rewards_current_episode += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # Exploration rate decay\n",
    "            exploration_rate = min_exploration_rate + \\\n",
    "                (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "\n",
    "            rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "            # Save optimal policy\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                action = env.get_best_action()\n",
    "                n_state, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                state = n_state\n",
    "\n",
    "                if done:\n",
    "                    if env.goal_reached():\n",
    "                        path_history.append(env.get_id_path(env.agent_path))\n",
    "\n",
    "                    policy_history.append(score)\n",
    "                    break\n",
    "        \n",
    "        optimal_reward = policy_history[-1]\n",
    "        writer.writerow({'alpha' : a, 'gamma' : g, 'optimal_reward' : optimal_reward, 'matching_nodes' : env.get_matching_nodes(sample_paths[2], env.get_id_path(env.agent_path)), 'length_diff' : env.get_length_diff(sample_paths[2], env.get_id_path(env.agent_path)), 'agent_path' : env.get_id_path(env.agent_path)})\n",
    "\n",
    "data.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c063398c",
   "metadata": {},
   "source": [
    "# Calculate Average Reward per Hundred Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_hundred_episodes = np.split(np.array(rewards_all_episodes), num_episodes / 100)\n",
    "count = 100\n",
    "\n",
    "for r in rewards_per_hundred_episodes:\n",
    "    print(count, \":\", round(sum(r / 100), 1))\n",
    "    count += 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf4f7b",
   "metadata": {},
   "source": [
    "# Print Updated Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06c824",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f31bc",
   "metadata": {},
   "source": [
    "# Save Updated Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4631d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ENTER_NAME_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165552d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(filename, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac7c19",
   "metadata": {},
   "source": [
    "# Load Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acfe0f1",
   "metadata": {},
   "source": [
    "# Visualize Optimal Policy Progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8933fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_paths = []\n",
    "\n",
    "for p in path_history:\n",
    "    if p not in unique_paths:\n",
    "        unique_paths.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "\n",
    "for p in path_history:\n",
    "    if unique_paths.index(p) in frequency:\n",
    "        frequency[unique_paths.index(p)] += 1\n",
    "    else:\n",
    "        frequency[unique_paths.index(p)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59024be",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c830bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = list(frequency.values())\n",
    "top_index = vals.index(max(vals))\n",
    "\n",
    "print('Most frequent path:', unique_paths[top_index])\n",
    "print('Frequency:', frequency[top_index])\n",
    "\n",
    "ox.plot.plot_graph_route(graph, unique_paths[top_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = path_history[len(path_history) - 1]\n",
    "\n",
    "print('Final path:', final_path)\n",
    "print('Frequency:', frequency[unique_paths.index(final_path)])\n",
    "\n",
    "ox.plot.plot_graph_route(graph, final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6089a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in frequency:\n",
    "    print('Path:', unique_paths[key])\n",
    "    print('Frequency:', frequency[key])\n",
    "    \n",
    "    ox.plot.plot_graph_route(graph, unique_paths[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbdc7c9",
   "metadata": {},
   "source": [
    "# Graph Rewards by Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(policy_history)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_history.index(max(policy_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f984faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards_all_episodes)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes.index(max(rewards_all_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597d5f3",
   "metadata": {},
   "source": [
    "# Replay Episode From History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7bc19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step_history = episode_history[0]\n",
    "max_steps = 1000\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "\n",
    "clear_output(wait = True)\n",
    "env.render()\n",
    "time.sleep(0.5)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    action = step_history[step]\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    state = n_state\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        print('Episode finished. Score:', round(score, 3))\n",
    "        time.sleep(2)\n",
    "        break\n",
    "\n",
    "if not done:\n",
    "    print('Goal not reached in {} steps.'.format(max_steps), 'Score:', round(score, 3))\n",
    "    time.sleep(2)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04e6bd",
   "metadata": {},
   "source": [
    "# Render Environment and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd6697",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episodes = 1\n",
    "max_steps = 1000\n",
    "history = []\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = env.get_best_action()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        state = n_state\n",
    "        \n",
    "        clear_output(wait = True)\n",
    "        env.render()\n",
    "        print(env.get_q_vals())\n",
    "        \n",
    "        if done:\n",
    "            print('Episode finished.')\n",
    "            print('Goal time:', round(env.goal_time, 2))\n",
    "            print('Time elapsed:', round(env.time_elapsed, 2))\n",
    "            print('Score:', round(score, 2))\n",
    "            time.sleep(2)\n",
    "            break\n",
    "            \n",
    "    if not done:\n",
    "        print('Goal not reached in {} steps.'.format(max_steps), 'Score:', round(score, 1))\n",
    "        time.sleep(2)\n",
    "    \n",
    "    history.append(score)\n",
    "\n",
    "print('Average score:', round(sum(history) / len(history), 2))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da062f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample path:')\n",
    "ox.plot.plot_graph_route(graph, env.PATH)\n",
    "\n",
    "print('Agent path:')\n",
    "ox.plot.plot_graph_route(graph, env.get_id_path(env.agent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.get_path_accuracy(env.PATH, env.get_id_path(env.agent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba79613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
